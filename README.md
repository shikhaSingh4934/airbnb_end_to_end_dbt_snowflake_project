# Airbnb End-to-End Data Engineering Project (dbt + Snowflake)

## ğŸ“Œ Project Overview

This project demonstrates an **end-to-end analytics engineering pipeline** built using **Snowflake** and **dbt**, following modern **lakehouse-style layered modeling (Bronze â†’ Silver â†’ Gold)**.

The goal of the project is to ingest raw Airbnb data from **Amazon S3**, stage it in Snowflake, and build **analytics-ready, scalable, and maintainable models** using dbt with incremental loading, upserts, metadata-driven design, and dimensional modeling (SCD Type 2).

---

## ğŸ—ï¸ Architecture

**Source â†’ Snowflake â†’ dbt Transformations â†’ Analytics Models**

```
S3 Bucket
   â†“ (External Stage + Storage Integration)
Snowflake Staging Schema
   â†“
Bronze Layer (Raw, Incremental)
   â†“
Silver Layer (Cleaned, Upserted, Enriched)
   â†“
Gold Layer
   â”œâ”€â”€ One Big Table (OBT â€“ metadata-driven)
   â””â”€â”€ Star Schema (Facts + SCD Type 2 Dimensions)
```

---

## ğŸ”Œ Data Ingestion (S3 â†’ Snowflake)

* Airbnb data is stored in an **Amazon S3 bucket**
* A **Snowflake Storage Integration** and **External Stage** are created
* Data is loaded into Snowflake **staging schema** using Snowflake-managed access
* dbt references these staged tables as sources

This approach ensures:

* Secure access using IAM roles
* No hard-coded credentials
* Scalable ingestion pattern

---

## ğŸ¥‰ Bronze Layer (Raw Incremental Models)

The **Bronze layer** represents raw, lightly processed data loaded incrementally from Snowflake sources.

### Models

* `bronze_bookings.sql`
* `bronze_listings.sql`
* `bronze_hosts.sql`

### Key Characteristics

* Incremental models using `materialized='incremental'`
* Source-aligned schemas
* Minimal transformations
* Primary keys used for deduplication

Purpose:

> Preserve raw data while enabling efficient incremental ingestion.

---

## ğŸ¥ˆ Silver Layer (Cleaned & Enriched Data)

The **Silver layer** applies business logic, data quality improvements, and upsert logic.

### Models

* `silver_bookings.sql`
* `silver_listings.sql`
* `silver_hosts.sql`

### Key Features

* **Upserts using incremental + unique_key**
* Column-level transformations
* Business-derived fields

### Example Transformation

In `silver_bookings.sql`, a new column is derived:

* `total_booking_amount` = base price + service fee + cleaning fee

Similar enrichment logic is applied across listings and hosts.

Purpose:

> Produce clean, trusted, and analytics-ready datasets.

---

## ğŸ¥‡ Gold Layer â€“ Analytics Models

The **Gold layer** contains business-consumable models optimized for analytics and reporting.

### 1ï¸âƒ£ One Big Table (OBT)

* Implemented in `obt.sql`
* Built using a **metadata-driven approach**

#### Why metadata-driven?

* Avoids hard-coded joins
* Easily extensible when new tables are introduced
* Only configuration changes required to add new datasets

#### Approach

* Table, column selection, aliases, and join conditions are stored in a Jinja config structure
* dbt dynamically generates the SELECT and JOIN logic

This enables scalable, maintainable modeling without rewriting SQL.

---

### 2ï¸âƒ£ Star Schema with SCD Type 2

This layer supports historical analysis and dimensional modeling.

#### Steps

1. **Ephemeral Models**

   * Created from the OBT
   * Select only required columns for each dimension
   * Improves reuse and reduces duplication

2. **Dimension Tables**

   * Built using dbt **snapshots**
   * Implement **Slowly Changing Dimension (SCD) Type 2**
   * Track historical changes for listings and hosts

3. **Fact Table**

   * Built using a **metadata-driven join approach**
   * Joins fact data with dimension tables

Purpose:

> Enable time-travel analysis, historical tracking, and BI-friendly schemas.

---

## ğŸ§ª Data Quality & Testing

dbt tests are implemented to ensure reliability:

* `not_null`
* `unique`
* Referential integrity checks


---

## ğŸ§° dbt Macros

Reusable macros were created to standardize transformations:

* **Multiply macro** â€“ supports precision-based calculations
* **Tag macro** â€“ categorizes values into business-friendly buckets
* **Trimmer macro** â€“ cleans string columns consistently

Macros improve:

* Reusability
* Readability
* Consistency across models

---

## ğŸ› ï¸ Tools & Technologies

* **Snowflake** â€“ Cloud data warehouse
* **dbt Core** â€“ Transformations & modeling
* **Amazon S3** â€“ Raw data storage
* **SQL & Jinja** â€“ Transformations and templating
* **Git & GitHub** â€“ Version control

---

## ğŸš€ Key Highlights

* End-to-end ELT pipeline
* Incremental loading & upserts
* Metadata-driven modeling
* Dimensional modeling with SCD Type 2
* Production-grade dbt practices

---

## ğŸ“ˆ Future Enhancements

* Add dbt exposures for BI tools
* Automate runs using Airflow or GitHub Actions
* Introduce data freshness checks
* Expand metrics layer

---

## ğŸ“„ Notes

* `profiles.yml` is intentionally excluded for security reasons
* Secrets are managed using environment variables

---

âœ… This project reflects real-world analytics engineering practices using dbt and Snowflake.
